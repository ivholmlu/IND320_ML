{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from credentials import config\n",
    "from authentication import get_token\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for extracting yearly data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_locality(config, token, year, week):\n",
    "    \"\"\"Get data about locality for a given year and week using /v1/geodata/fishhealth/locality/{year}/{week}\n",
    "\n",
    "    Args:\n",
    "        year (int): year to extract data\n",
    "        week (int): week to extract data\n",
    "\n",
    "    Returns:\n",
    "        list: list of dictionaries containing the locality data\n",
    "    \"\"\"\n",
    "    url = f\"{config['api_base_url']}/v1/geodata/fishhealth/locality/{year}/{week}\"\n",
    "    headers ={\n",
    "    'authorization': 'Bearer ' + token['access_token'],\n",
    "    'content-type': 'application/json',}\n",
    "    r = requests.get(url, headers=headers)\n",
    "    return r.json() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token request successful\n"
     ]
    }
   ],
   "source": [
    "def get_week_summary(token, year, week):\n",
    "  url = f\"{config['api_base_url']}/v1/geodata/fishhealth/locality/{year}/{week}\"\n",
    "  headers ={\n",
    "    'authorization': 'Bearer ' + token['access_token'],\n",
    "    'content-type': 'application/json',\n",
    "  }\n",
    "\n",
    "  response = requests.get(url, headers=headers)\n",
    "  response.raise_for_status()\n",
    "  return response.json()\n",
    "token = get_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to cassandra\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect() #Kan vel være et argument til funksjonen? Tenkte på dette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = r\"/home/ivholmlu/miniconda3/envs/ind320/bin/python\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = r\"/home/ivholmlu/miniconda3/envs/ind320/bin/python\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/11/27 21:21:05 WARN Utils: Your hostname, Ivars-PC resolves to a loopback address: 127.0.1.1; using 172.23.6.83 instead (on interface eth0)\n",
      "23/11/27 21:21:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ivholmlu/miniconda3/envs/ind320/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ivholmlu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ivholmlu/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2ba2a0fc-2de2-4796-9834-a43dde6f9459;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      ":: resolution report :: resolve 487ms :: artifacts dl 16ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.1 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   19  |   0   |   0   |   0   ||   19  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2ba2a0fc-2de2-4796-9834-a43dde6f9459\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 19 already retrieved (0kB/11ms)\n",
      "23/11/27 21:21:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.appName('SparkCassandraApp').\\\n",
    "        config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.4.1').\\\n",
    "        config('spark.cassandra.connection.host', 'localhost').\\\n",
    "        config('spark.sql.extensions', 'com.datastax.spark.connector.CassandraSparkExtensions').\\\n",
    "        config('spark.sql.catalog.mycatalog', 'com.datastax.spark.connector.datasource.CassandraCatalog').\\\n",
    "        config(\"spark.cassandra.output.batch.size.rows\", \"1000\").\\\n",
    "        config('spark.cassandra.connection.port', '9042').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week:  1--- 0.7958188056945801 seconds\n",
      "Week:  2--- 0.5920321941375732 seconds\n",
      "Week:  3--- 0.7669072151184082 seconds\n",
      "Week:  4--- 0.6881039142608643 seconds\n",
      "Week:  5--- 0.7927930355072021 seconds\n",
      "Week:  6--- 0.7347080707550049 seconds\n",
      "Week:  7--- 0.6335101127624512 seconds\n",
      "Week:  8--- 0.6371841430664062 seconds\n",
      "Week:  9--- 0.7171177864074707 seconds\n",
      "Week: 10--- 0.6663081645965576 seconds\n",
      "Week: 11--- 0.5716259479522705 seconds\n",
      "Week: 12--- 0.5859527587890625 seconds\n",
      "Week: 13--- 0.6941020488739014 seconds\n",
      "Week: 14--- 0.7057192325592041 seconds\n",
      "Week: 15--- 0.6243369579315186 seconds\n",
      "Week: 16--- 0.6132721900939941 seconds\n",
      "Week: 17--- 0.533735990524292 seconds\n",
      "Week: 18--- 0.675762414932251 seconds\n",
      "Week: 19--- 0.768423318862915 seconds\n",
      "Week: 20--- 0.7453799247741699 seconds\n",
      "Week: 21--- 0.6051833629608154 seconds\n",
      "Week: 22--- 0.7376117706298828 seconds\n",
      "Week: 23--- 0.6377501487731934 seconds\n",
      "Week: 24--- 0.6657505035400391 seconds\n",
      "Week: 25--- 0.5844776630401611 seconds\n",
      "Week: 26--- 0.5787780284881592 seconds\n",
      "Week: 27--- 0.6460628509521484 seconds\n",
      "Week: 28--- 0.6133637428283691 seconds\n",
      "Week: 29--- 0.5029053688049316 seconds\n",
      "Week: 30--- 0.6024773120880127 seconds\n",
      "Week: 31--- 0.586350679397583 seconds\n",
      "Week: 32--- 0.6166996955871582 seconds\n",
      "Week: 33--- 0.616295576095581 seconds\n",
      "Week: 34--- 0.5918371677398682 seconds\n",
      "Week: 35--- 0.5895395278930664 seconds\n",
      "Week: 36--- 0.5847890377044678 seconds\n",
      "Week: 37--- 0.616365909576416 seconds\n",
      "Week: 38--- 0.5214169025421143 seconds\n",
      "Week: 39--- 0.6097712516784668 seconds\n",
      "Week: 40--- 0.5091180801391602 seconds\n",
      "Week: 41--- 0.48856472969055176 seconds\n",
      "Week: 42--- 0.5166516304016113 seconds\n",
      "Week: 43--- 0.546654224395752 seconds\n",
      "Week: 44--- 0.5534543991088867 seconds\n",
      "Week: 45--- 0.5468645095825195 seconds\n",
      "Week: 46--- 0.6877796649932861 seconds\n",
      "Week: 47--- 0.6172645092010498 seconds\n",
      "Week: 48--- 0.6296594142913818 seconds\n",
      "Week: 49--- 0.5704820156097412 seconds\n",
      "Week: 50--- 0.5670044422149658 seconds\n",
      "Week: 51--- 0.5190262794494629 seconds\n",
      "Week: 52--- 0.5157303810119629 seconds\n",
      "(86358, 22)\n",
      "Total time: 44.6114616394043 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Take time the cell use to run and print in the end\n",
    "#Print the time used so far in seconds after print(week)\n",
    "year = 2018\n",
    "start_time = time.time()\n",
    "list_localitites = []\n",
    "year_in_db = check_year(year)\n",
    "from functions import get_week_summary\n",
    "\n",
    "else:\n",
    "    for week in range(1, 53):\n",
    "        start_it = time.time() \n",
    "\n",
    "        locality_data = get_week_summary(token, year, week)\n",
    "        for row in locality_data[\"localities\"]:\n",
    "            row[\"year\"] = locality_data[\"year\"]\n",
    "            row[\"week\"] = locality_data[\"week\"]\n",
    "\n",
    "            dt = pd.Timestamp.fromisocalendar(locality_data[\"year\"], locality_data[\"week\"], 1)\n",
    "            dt = dt.tz_localize('UTC')\n",
    "            row[\"datetime\"] = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            list_localitites.append(row)\n",
    "        print(f\"Week: {week:>2}--- %s seconds\" % (time.time() - start_it))\n",
    "        df_localities_year = pd.DataFrame(list_localitites)\n",
    "        df_localities_year.columns = map(str.lower, df_localities_year.columns)\n",
    "        \n",
    "        \n",
    "    print(df_localities_year.shape)\n",
    "\n",
    "    print(\"Total time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>localityno</th>\n",
       "      <th>localityweekid</th>\n",
       "      <th>name</th>\n",
       "      <th>hasreportedlice</th>\n",
       "      <th>isfallow</th>\n",
       "      <th>avgadultfemalelice</th>\n",
       "      <th>hascleanerfishdeployed</th>\n",
       "      <th>hasmechanicalremoval</th>\n",
       "      <th>hassubstancetreatments</th>\n",
       "      <th>haspd</th>\n",
       "      <th>...</th>\n",
       "      <th>municipality</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>isonland</th>\n",
       "      <th>infilteredselection</th>\n",
       "      <th>hassalmonoids</th>\n",
       "      <th>isslaughterholdingcage</th>\n",
       "      <th>year</th>\n",
       "      <th>week</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14746</td>\n",
       "      <td>713526</td>\n",
       "      <td>Aarsand</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Bindal</td>\n",
       "      <td>65.045867</td>\n",
       "      <td>12.156933</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31937</td>\n",
       "      <td>717296</td>\n",
       "      <td>Abelsnes</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Flekkefjord</td>\n",
       "      <td>58.238767</td>\n",
       "      <td>6.656650</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10665</td>\n",
       "      <td>645029</td>\n",
       "      <td>Adamselv</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Lebesby</td>\n",
       "      <td>70.408000</td>\n",
       "      <td>26.691333</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29196</td>\n",
       "      <td>837182</td>\n",
       "      <td>Adjetjohka</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Guovdageaidnu-Kautokeino</td>\n",
       "      <td>68.944137</td>\n",
       "      <td>22.918715</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13823</td>\n",
       "      <td>645374</td>\n",
       "      <td>Ådlandsvatn</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Stord</td>\n",
       "      <td>59.791350</td>\n",
       "      <td>5.500367</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-31 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   localityno  localityweekid         name  hasreportedlice  isfallow  \\\n",
       "0       14746          713526      Aarsand            False      True   \n",
       "1       31937          717296     Abelsnes            False      True   \n",
       "2       10665          645029     Adamselv            False      True   \n",
       "3       29196          837182   Adjetjohka            False      True   \n",
       "4       13823          645374  Ådlandsvatn            False      True   \n",
       "\n",
       "   avgadultfemalelice  hascleanerfishdeployed  hasmechanicalremoval  \\\n",
       "0                 NaN                   False                 False   \n",
       "1                 NaN                   False                 False   \n",
       "2                 NaN                   False                 False   \n",
       "3                 NaN                   False                 False   \n",
       "4                 NaN                   False                 False   \n",
       "\n",
       "   hassubstancetreatments  haspd  ...              municipality        lat  \\\n",
       "0                   False  False  ...                    Bindal  65.045867   \n",
       "1                   False  False  ...               Flekkefjord  58.238767   \n",
       "2                   False  False  ...                   Lebesby  70.408000   \n",
       "3                   False  False  ...  Guovdageaidnu-Kautokeino  68.944137   \n",
       "4                   False  False  ...                     Stord  59.791350   \n",
       "\n",
       "         lon  isonland  infilteredselection  hassalmonoids  \\\n",
       "0  12.156933     False                 True          False   \n",
       "1   6.656650      True                 True          False   \n",
       "2  26.691333      True                 True           True   \n",
       "3  22.918715      True                 True          False   \n",
       "4   5.500367      True                 True           True   \n",
       "\n",
       "   isslaughterholdingcage  year  week             datetime  \n",
       "0                   False  2019     1  2018-12-31 00:00:00  \n",
       "1                   False  2019     1  2018-12-31 00:00:00  \n",
       "2                   False  2019     1  2018-12-31 00:00:00  \n",
       "3                   False  2019     1  2018-12-31 00:00:00  \n",
       "4                   False  2019     1  2018-12-31 00:00:00  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_localities_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append pd dataframe to cassandra table using pyspark\n",
    "spark_df_localities_year = spark.createDataFrame(df_localities_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_localities_year.write\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .mode('append')\\\n",
    "    .options(table=\"localities\", keyspace=\"fish_data\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'C:\\Users\\holml\\OneDrive\\Documents\\GitHub\\IND320\\ca4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Source\\IND320_ML\\ca4.ipynb Cell 21\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Source/IND320_ML/ca4.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m pd_df_localities_year\u001b[39m.\u001b[39mshape\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Source/IND320_ML/ca4.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#Save dataframe to csv\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Source/IND320_ML/ca4.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m pd_df_localities_year\u001b[39m.\u001b[39;49mto_csv(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mholml\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mOneDrive\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDocuments\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mGitHub\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mIND320\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mca4\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mlocalities.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, index \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\holml\\anaconda3\\envs\\IND320_ml\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\holml\\anaconda3\\envs\\IND320_ml\\Lib\\site-packages\\pandas\\core\\generic.py:3720\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3709\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[0;32m   3711\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3712\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[0;32m   3713\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3717\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[0;32m   3718\u001b[0m )\n\u001b[1;32m-> 3720\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[0;32m   3721\u001b[0m     path_or_buf,\n\u001b[0;32m   3722\u001b[0m     lineterminator\u001b[39m=\u001b[39;49mlineterminator,\n\u001b[0;32m   3723\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[0;32m   3724\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   3725\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   3726\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   3727\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[0;32m   3728\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   3729\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[0;32m   3730\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m   3731\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m   3732\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[0;32m   3733\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m   3734\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[0;32m   3735\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[0;32m   3736\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   3737\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\holml\\anaconda3\\envs\\IND320_ml\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\holml\\anaconda3\\envs\\IND320_ml\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1168\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1171\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1172\u001b[0m     lineterminator\u001b[39m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[0;32m   1188\u001b[0m )\n\u001b[1;32m-> 1189\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m   1191\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1192\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\holml\\anaconda3\\envs\\IND320_ml\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m    242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[0;32m    243\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    244\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    245\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[0;32m    246\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[0;32m    247\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[0;32m    248\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\holml\\anaconda3\\envs\\IND320_ml\\Lib\\site-packages\\pandas\\io\\common.py:734\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[39m# Only for write methods\u001b[39;00m\n\u001b[0;32m    733\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m is_path:\n\u001b[1;32m--> 734\u001b[0m     check_parent_directory(\u001b[39mstr\u001b[39;49m(handle))\n\u001b[0;32m    736\u001b[0m \u001b[39mif\u001b[39;00m compression:\n\u001b[0;32m    737\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzstd\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    738\u001b[0m         \u001b[39m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\holml\\anaconda3\\envs\\IND320_ml\\Lib\\site-packages\\pandas\\io\\common.py:597\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    595\u001b[0m parent \u001b[39m=\u001b[39m Path(path)\u001b[39m.\u001b[39mparent\n\u001b[0;32m    596\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parent\u001b[39m.\u001b[39mis_dir():\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mrf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot save file into a non-existent directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparent\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'C:\\Users\\holml\\OneDrive\\Documents\\GitHub\\IND320\\ca4'"
     ]
    }
   ],
   "source": [
    "# Show fish_data.localities\n",
    "spark_df_localities_year = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"localities\", keyspace=\"fish_data\").load()\n",
    "pd_df_localities_year = spark_df_localities_year.toPandas()\n",
    "pd_df_localities_year.shape\n",
    "#Save dataframe to csv\n",
    "pd_df_localities_year.to_csv(r'C:\\Users\\holml\\OneDrive\\Documents\\GitHub\\IND320_ML\\data\\localities.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df_localities_year.to_csv(r'data\\localities.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|year|\n",
      "+----+\n",
      "|2018|\n",
      "|2019|\n",
      "|2020|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show size of spark dataframe\n",
    "unique_values = spark_df_localities_year.select('year').distinct()\n",
    "unique_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_localities_year = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"localities\", keyspace=\"fish_data\").load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ind320_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
